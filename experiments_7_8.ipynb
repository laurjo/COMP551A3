{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9c6f4c-2946-4835-b873-39f98883e8dd",
   "metadata": {},
   "source": [
    "Experiment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3cf4a9-c587-499c-b22d-ef2b99f2dcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████████████████████████| 938/938 [00:41<00:00, 22.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 692.1147, Train Acc: 72.21%, Test Acc: 81.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████| 938/938 [00:40<00:00, 23.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 515.5341, Train Acc: 79.53%, Test Acc: 84.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████| 938/938 [00:40<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 449.9911, Train Acc: 82.34%, Test Acc: 86.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████████████████████████| 938/938 [00:40<00:00, 23.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 415.8026, Train Acc: 83.61%, Test Acc: 85.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████| 938/938 [00:41<00:00, 22.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 391.9721, Train Acc: 84.40%, Test Acc: 88.01%\n",
      "\n",
      "Total training time for Experiment 7: 212.77 seconds\n",
      "Final Test Accuracy for Experiment 7: 0.8801\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"Computing mean and std of the dataset\"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        # Mean over batch, height, and width, for each channel\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = torch.sqrt(channels_squared_sum / num_batches - mean ** 2)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "#load raw training dataset (without normalization) to compute stats\n",
    "temp_train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#Transforms with normalization and data augmentation from exp5\n",
    "train_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=train_mean, std=train_std),\n",
    "    # Data augmentation\n",
    "    v2.RandomCrop(size=(28, 28), padding=4),\n",
    "    v2.RandomRotation(degrees=(-10, 10)),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=train_mean, std=train_std)\n",
    "])\n",
    "\n",
    "#Datasets and loaders\n",
    "full_train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "train_dataset = full_train_dataset\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#CNN from exp6\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#Training\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    #Evaluating on test set\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "    test_acc = correct_test / total_test\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {total_loss:.4f}, Train Acc: {train_acc:.2%}, Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTotal training time for Experiment 7: {total_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Final Test Accuracy for Experiment 7: {test_accuracies[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfd203-b38f-414e-ab7c-43770928f169",
   "metadata": {},
   "source": [
    "Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a7e2c93-4d78-4e09-a084-c4c60e21721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████████████████████████| 938/938 [03:24<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 1262.8518, Train Acc: 50.80%, Test Acc: 60.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████████████████████████| 938/938 [03:37<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train Loss: 1123.4619, Train Acc: 56.32%, Test Acc: 62.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████████████████████████| 938/938 [03:26<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train Loss: 1087.3381, Train Acc: 57.66%, Test Acc: 62.70%\n",
      "\n",
      "Total training time for Experiment 8: 722.31 seconds\n",
      "Final Test Accuracy for Experiment 8: 0.6270\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"Computing mean and std of the dataset.\"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = torch.sqrt(channels_squared_sum / num_batches - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# raw training set to get stats \n",
    "temp_train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# 3-channel stats for ResNet\n",
    "mean_val = float(train_mean.item())\n",
    "std_val = float(train_std.item())\n",
    "mean_3c = [mean_val, mean_val, mean_val]\n",
    "std_3c = [std_val, std_val, std_val]\n",
    "\n",
    "\n",
    "# data augmentation from Q5\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Lambda(lambda x: x.repeat(3, 1, 1)),               \n",
    "    v2.Normalize(mean=mean_3c, std=std_3c),\n",
    "    v2.RandomCrop(size=(28, 28), padding=4),\n",
    "    v2.RandomRotation(degrees=(-10, 10)),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    v2.Normalize(mean=mean_3c, std=std_3c),\n",
    "])\n",
    "\n",
    "#datasets and loaders\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform,\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "#Pre-trained ResNet and configurable fully connected layers\n",
    "\n",
    "\n",
    "base_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "backbone_in_features = base_model.fc.in_features \n",
    "\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "base_model.fc = nn.Identity()\n",
    "\n",
    "\n",
    "class PretrainedFashionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-trained ResNet + configurable fully connected classifier.\n",
    "    only the newly added fully connected layers will be trained.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, in_features, num_classes=10,\n",
    "                 num_fc_layers=2, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        layers = []\n",
    "        input_dim = in_features\n",
    "\n",
    "        \n",
    "        for _ in range(max(0, num_fc_layers - 1)):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "\n",
    "        \n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = PretrainedFashionNet(\n",
    "    backbone=base_model,\n",
    "    in_features=backbone_in_features,\n",
    "    num_classes=10,      \n",
    "    num_fc_layers=3,     # number of fully connected layers (changed between 1 and 3)\n",
    "    hidden_size=256\n",
    ").to(device)\n",
    "\n",
    "# only training the new fully connected layers\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = Adam(model.classifier.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#training loop \n",
    "\n",
    "num_epochs = 3 \n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # evaluating on test set\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "    test_acc = correct_test / total_test\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "        f\"Train Loss: {total_loss:.4f}, Train Acc: {train_acc:.2%}, \"\n",
    "        f\"Test Acc: {test_acc:.2%}\"\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nTotal training time for Experiment 8: {total_time:.2f} seconds\")\n",
    "print(f\"Final Test Accuracy for Experiment 8: {test_accuracies[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4d63d-2eb2-4b4f-ab5e-2a2d142be429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
