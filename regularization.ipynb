{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbe7b4c",
   "metadata": {},
   "source": [
    "Produce L1 Regularization Performance Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e2d66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the arrays\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "#the validation set could be used for cross-validation if we decide to implement it\n",
    "#X_val = np.load('X_val.npy')\n",
    "#y_val = np.load('y_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "#print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "#activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, gamma=0.01):\n",
    "    return np.maximum(0, x) + gamma * np.minimum(0, x)\n",
    "\n",
    "def leaky_relu_grad(x, gamma=0.01):\n",
    "    grad = np.ones_like(x)\n",
    "    grad[x < 0] = gamma\n",
    "    return grad\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: (batch_size, num_classes)\n",
    "    y_true: (batch_size,)\n",
    "    \"\"\"\n",
    "    n = y_true.shape[0]\n",
    "    #convert labels to one-hot encoding\n",
    "    y_one_hot = np.zeros_like(y_pred)\n",
    "    y_one_hot[np.arange(n), y_true] = 1\n",
    "    #compute cross-entropy loss\n",
    "    loss = -np.sum(y_one_hot * np.log(y_pred + 1e-8)) / n\n",
    "    return loss\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, h, depth, m, gamma=0.01, l1 = False, l2 = False):\n",
    "        \"\"\"\n",
    "        h: activation function\n",
    "        depth: number of hidden layers\n",
    "        m: number of units per hidden layer\n",
    "        gamma: parameter for leaky_relu\n",
    "        \"\"\"\n",
    "        self.h = h\n",
    "        self.depth = depth\n",
    "        self.m = m\n",
    "        self.gamma = gamma\n",
    "        self.add_l1 = l1\n",
    "        self.add_l2 = l2\n",
    "        \n",
    "        #determine activation gradient function\n",
    "        if h == relu:\n",
    "            self.h_grad = relu_grad\n",
    "        elif h == leaky_relu:\n",
    "            self.h_grad = lambda x: leaky_relu_grad(x, gamma)\n",
    "        elif h == tanh:\n",
    "            self.h_grad = tanh_grad\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "        \n",
    "        #initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        #determine initialization scale\n",
    "        def get_init_scale(n_in, n_out):\n",
    "            if h == relu or h == leaky_relu:\n",
    "                return np.sqrt(2 / n_in)  #He initialization\n",
    "            else:  #tanh\n",
    "                return np.sqrt(1 / (n_in + n_out))  #Xavier initialization\n",
    "        \n",
    "        #input to first hidden layer\n",
    "        n_in = 784\n",
    "        if depth == 0:\n",
    "            n_out = 10\n",
    "            scale = np.sqrt(1 / (n_in + n_out))  # Xavier for output layer\n",
    "            self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "            self.biases.append(np.zeros((1, n_out)))\n",
    "        else:\n",
    "            #first hidden layer\n",
    "            n_out = m\n",
    "            scale = get_init_scale(n_in, n_out)\n",
    "            self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "            self.biases.append(np.zeros((1, n_out)))\n",
    "            \n",
    "            #init additional hidden layers\n",
    "            for i in range(depth - 1):\n",
    "                n_in = m\n",
    "                n_out = m\n",
    "                scale = get_init_scale(n_in, n_out)\n",
    "                self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "                self.biases.append(np.zeros((1, n_out)))\n",
    "            \n",
    "            #init output layer\n",
    "            n_in = m\n",
    "            n_out = 10\n",
    "            scale = np.sqrt(1 / (n_in + n_out))  # Xavier for softmax output\n",
    "            self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "            self.biases.append(np.zeros((1, n_out)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        x: (batch_size, 784)\n",
    "        Returns: predictions (batch_size, 10), cache for backprop\n",
    "        \"\"\"\n",
    "        cache = {'activations': [x], 'pre_activations': []}\n",
    "        \n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            cache['pre_activations'].append(z)\n",
    "            \n",
    "            if self.h == leaky_relu:\n",
    "                a = self.h(z, self.gamma)\n",
    "            else:\n",
    "                a = self.h(z)\n",
    "            cache['activations'].append(a)\n",
    "        \n",
    "        #output layer with softmax\n",
    "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        cache['pre_activations'].append(z)\n",
    "        y_pred = softmax(z)\n",
    "        cache['activations'].append(y_pred)\n",
    "        \n",
    "        return y_pred, cache\n",
    "    \n",
    "    def backward(self, y_pred, y_true, cache, reg_coeff):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients\n",
    "        y_pred: (batch_size, 10)\n",
    "        y_true: (batch_size,)\n",
    "        cache: forward pass cache\n",
    "        \"\"\"\n",
    "        n = y_true.shape[0]\n",
    "        \n",
    "        #convert labels to one-hot\n",
    "        y_one_hot = np.zeros_like(y_pred)\n",
    "        y_one_hot[np.arange(n), y_true] = 1\n",
    "        \n",
    "        dz = (y_pred - y_one_hot) / n\n",
    "        \n",
    "        grads_w = []\n",
    "        grads_b = []\n",
    "        \n",
    "        # Backprop through layers in reverse\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            #gradient of weights and biases\n",
    "            a_prev = cache['activations'][i]\n",
    "            dw = np.dot(a_prev.T, dz)\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            \n",
    "            if self.add_l2:\n",
    "                grads_w.insert(0, dw + reg_coeff * self.weights[i])\n",
    "            elif self.add_l1:\n",
    "                grads_w.insert(0, dw + reg_coeff * np.sign(self.weights[i]))\n",
    "            else:\n",
    "                grads_w.insert(0, dw)\n",
    "\n",
    "            grads_b.insert(0, db)\n",
    "            \n",
    "            #gradient of previous layer activation\n",
    "            if i > 0:\n",
    "                da = np.dot(dz, self.weights[i].T)\n",
    "                #gradient through activation function\n",
    "                z_prev = cache['pre_activations'][i - 1]\n",
    "                dz = da * self.h_grad(z_prev)\n",
    "\n",
    "            \n",
    "        \n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, learning_rate=0.01, epochs=100, batch_size=64, reg_coeff=0):\n",
    "        \"\"\"\n",
    "        Train the MLP using mini-batch gradient descent\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #shuffle training data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            #mini-batch gradient descent\n",
    "            for batch in range(n_batches):\n",
    "                start = batch * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred, cache = self.forward(X_batch)\n",
    "\n",
    "                #compute loss\n",
    "                loss = cross_entropy_loss(y_pred, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                #backward pass\n",
    "                grads_w, grads_b = self.backward(y_pred, y_batch, cache, reg_coeff)\n",
    "\n",
    "                #update weights and biases\n",
    "                for i in range(len(self.weights)):\n",
    "                    self.weights[i] -= learning_rate * grads_w[i]\n",
    "                    self.biases[i] -= learning_rate * grads_b[i]\n",
    "\n",
    "            #average loss over batches\n",
    "            epoch_loss /= n_batches\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "            yh = self.predict(X_train)\n",
    "            train_acc = self.evaluate_acc(y_train,yh)\n",
    "            train_accuracies.append(train_acc)\n",
    "\n",
    "            #test/validation\n",
    "            yh = self.predict(X_test)\n",
    "            test_acc = self.evaluate_acc(y_test,yh)\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "        return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions and compute accuracy\n",
    "        X: (n_samples, 784)\n",
    "        y: (n_samples,) - true labels\n",
    "        Returns: accuracy\n",
    "        \"\"\"\n",
    "        yh, _ = self.forward(X)\n",
    "        return yh\n",
    "\n",
    "    def evaluate_acc(self,y,yh):\n",
    "        predictions = np.argmax(yh, axis=1)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "\n",
    "def cross_validation_split(n, n_folds=5):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds\n",
    "\n",
    "def kfold_cross_val(x , y, n_folds , model, lr, n_epoches, reg_coeff):\n",
    "    score_val = np.zeros(n_folds)\n",
    "    for f, (tr, val) in enumerate(cross_validation_split(x.shape[0], n_folds)):\n",
    "        train_losses, train_accuracies, test_accuracies = model.fit(x[tr], y[tr], x[val], y[val], learning_rate=lr, epochs=n_epoches, reg_coeff=reg_coeff)\n",
    "        score_val[f] = model.evaluate_acc(y[val], model.predict(x[val]))\n",
    "    return score_val, score_val.mean()\n",
    "\n",
    "regularization_strengths = np.array([1, 10, 50])\n",
    "\n",
    "\n",
    "#plot final performance graphs\n",
    "for strength in regularization_strengths:\n",
    "    model = MLP(relu, depth=2, m=256, l1 = True)\n",
    "    train_losses, train_accuracies, test_accuracies = model.fit(X_train, y_train, X_test, y_test,learning_rate=0.01,epochs=100, reg_coeff=strength)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), layout='constrained')\n",
    "    ax.plot(test_accuracies, label=\"Testing Set\")\n",
    "    ax.plot(train_accuracies, label = \"Training Set\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"Strength = {strength}\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    yh_train = model.predict(X_train)\n",
    "    yh_test = model.predict(X_test)\n",
    "    test_acc = model.evaluate_acc(y_test, yh_test)\n",
    "    train_acc = model.evaluate_acc(y_train, yh_train)\n",
    "    print(f\"Regularization Strength: \" + str(strength))\n",
    "    print(f\"Final test acc = {test_acc}\")\n",
    "    print(f\"Final test acc = {train_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44aceee",
   "metadata": {},
   "source": [
    "Produce L2 Regularization Performance Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc30db9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the arrays\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "#the validation set could be used for cross-validation if we decide to implement it\n",
    "#X_val = np.load('X_val.npy')\n",
    "#y_val = np.load('y_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "#print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "#activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, gamma=0.01):\n",
    "    return np.maximum(0, x) + gamma * np.minimum(0, x)\n",
    "\n",
    "def leaky_relu_grad(x, gamma=0.01):\n",
    "    grad = np.ones_like(x)\n",
    "    grad[x < 0] = gamma\n",
    "    return grad\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: (batch_size, num_classes)\n",
    "    y_true: (batch_size,)\n",
    "    \"\"\"\n",
    "    n = y_true.shape[0]\n",
    "    #convert labels to one-hot encoding\n",
    "    y_one_hot = np.zeros_like(y_pred)\n",
    "    y_one_hot[np.arange(n), y_true] = 1\n",
    "    #compute cross-entropy loss\n",
    "    loss = -np.sum(y_one_hot * np.log(y_pred + 1e-8)) / n\n",
    "    return loss\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, h, depth, m, gamma=0.01, l1 = False, l2 = False):\n",
    "        \"\"\"\n",
    "        h: activation function\n",
    "        depth: number of hidden layers\n",
    "        m: number of units per hidden layer\n",
    "        gamma: parameter for leaky_relu\n",
    "        \"\"\"\n",
    "        self.h = h\n",
    "        self.depth = depth\n",
    "        self.m = m\n",
    "        self.gamma = gamma\n",
    "        self.add_l1 = l1\n",
    "        self.add_l2 = l2\n",
    "        \n",
    "        #determine activation gradient function\n",
    "        if h == relu:\n",
    "            self.h_grad = relu_grad\n",
    "        elif h == leaky_relu:\n",
    "            self.h_grad = lambda x: leaky_relu_grad(x, gamma)\n",
    "        elif h == tanh:\n",
    "            self.h_grad = tanh_grad\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "        \n",
    "        #initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        #determine initialization scale\n",
    "        def get_init_scale(n_in, n_out):\n",
    "            if h == relu or h == leaky_relu:\n",
    "                return np.sqrt(2 / n_in)  #He initialization\n",
    "            else:  #tanh\n",
    "                return np.sqrt(1 / (n_in + n_out))  #Xavier initialization\n",
    "        \n",
    "        #input to first hidden layer\n",
    "        n_in = 784\n",
    "        if depth == 0:\n",
    "            n_out = 10\n",
    "            scale = np.sqrt(1 / (n_in + n_out))  # Xavier for output layer\n",
    "            self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "            self.biases.append(np.zeros((1, n_out)))\n",
    "        else:\n",
    "            #first hidden layer\n",
    "            n_out = m\n",
    "            scale = get_init_scale(n_in, n_out)\n",
    "            self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "            self.biases.append(np.zeros((1, n_out)))\n",
    "            \n",
    "            #init additional hidden layers\n",
    "            for i in range(depth - 1):\n",
    "                n_in = m\n",
    "                n_out = m\n",
    "                scale = get_init_scale(n_in, n_out)\n",
    "                self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "                self.biases.append(np.zeros((1, n_out)))\n",
    "            \n",
    "            #init output layer\n",
    "            n_in = m\n",
    "            n_out = 10\n",
    "            scale = np.sqrt(1 / (n_in + n_out))  # Xavier for softmax output\n",
    "            self.weights.append(np.random.normal(0, scale, size=(n_in, n_out)))\n",
    "            self.biases.append(np.zeros((1, n_out)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        x: (batch_size, 784)\n",
    "        Returns: predictions (batch_size, 10), cache for backprop\n",
    "        \"\"\"\n",
    "        cache = {'activations': [x], 'pre_activations': []}\n",
    "        \n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            cache['pre_activations'].append(z)\n",
    "            \n",
    "            if self.h == leaky_relu:\n",
    "                a = self.h(z, self.gamma)\n",
    "            else:\n",
    "                a = self.h(z)\n",
    "            cache['activations'].append(a)\n",
    "        \n",
    "        #output layer with softmax\n",
    "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        cache['pre_activations'].append(z)\n",
    "        y_pred = softmax(z)\n",
    "        cache['activations'].append(y_pred)\n",
    "        \n",
    "        return y_pred, cache\n",
    "    \n",
    "    def backward(self, y_pred, y_true, cache, reg_coeff):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients\n",
    "        y_pred: (batch_size, 10)\n",
    "        y_true: (batch_size,)\n",
    "        cache: forward pass cache\n",
    "        \"\"\"\n",
    "        n = y_true.shape[0]\n",
    "        \n",
    "        #convert labels to one-hot\n",
    "        y_one_hot = np.zeros_like(y_pred)\n",
    "        y_one_hot[np.arange(n), y_true] = 1\n",
    "        \n",
    "        dz = (y_pred - y_one_hot) / n\n",
    "        \n",
    "        grads_w = []\n",
    "        grads_b = []\n",
    "        \n",
    "        # Backprop through layers in reverse\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            #gradient of weights and biases\n",
    "            a_prev = cache['activations'][i]\n",
    "            dw = np.dot(a_prev.T, dz)\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            \n",
    "            if self.add_l2:\n",
    "                grads_w.insert(0, dw + reg_coeff * self.weights[i])\n",
    "            elif self.add_l1:\n",
    "                grads_w.insert(0, dw + reg_coeff * np.sign(self.weights[i]))\n",
    "            else:\n",
    "                grads_w.insert(0, dw)\n",
    "\n",
    "            grads_b.insert(0, db)\n",
    "            \n",
    "            #gradient of previous layer activation\n",
    "            if i > 0:\n",
    "                da = np.dot(dz, self.weights[i].T)\n",
    "                #gradient through activation function\n",
    "                z_prev = cache['pre_activations'][i - 1]\n",
    "                dz = da * self.h_grad(z_prev)\n",
    "\n",
    "            \n",
    "        \n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, learning_rate=0.01, epochs=100, batch_size=64, reg_coeff=0):\n",
    "        \"\"\"\n",
    "        Train the MLP using mini-batch gradient descent\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #shuffle training data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            #mini-batch gradient descent\n",
    "            for batch in range(n_batches):\n",
    "                start = batch * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred, cache = self.forward(X_batch)\n",
    "\n",
    "                #compute loss\n",
    "                loss = cross_entropy_loss(y_pred, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                #backward pass\n",
    "                grads_w, grads_b = self.backward(y_pred, y_batch, cache, reg_coeff)\n",
    "\n",
    "                #update weights and biases\n",
    "                for i in range(len(self.weights)):\n",
    "                    self.weights[i] -= learning_rate * grads_w[i]\n",
    "                    self.biases[i] -= learning_rate * grads_b[i]\n",
    "\n",
    "            #average loss over batches\n",
    "            epoch_loss /= n_batches\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "            yh = self.predict(X_train)\n",
    "            train_acc = self.evaluate_acc(y_train,yh)\n",
    "            train_accuracies.append(train_acc)\n",
    "\n",
    "            #test/validation\n",
    "            yh = self.predict(X_test)\n",
    "            test_acc = self.evaluate_acc(y_test,yh)\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "        return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions and compute accuracy\n",
    "        X: (n_samples, 784)\n",
    "        y: (n_samples,) - true labels\n",
    "        Returns: accuracy\n",
    "        \"\"\"\n",
    "        yh, _ = self.forward(X)\n",
    "        return yh\n",
    "\n",
    "    def evaluate_acc(self,y,yh):\n",
    "        predictions = np.argmax(yh, axis=1)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "\n",
    "def cross_validation_split(n, n_folds=5):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds\n",
    "\n",
    "def kfold_cross_val(x , y, n_folds , model, lr, n_epoches, reg_coeff):\n",
    "    score_val = np.zeros(n_folds)\n",
    "    for f, (tr, val) in enumerate(cross_validation_split(x.shape[0], n_folds)):\n",
    "        train_losses, train_accuracies, test_accuracies = model.fit(x[tr], y[tr], x[val], y[val], learning_rate=lr, epochs=n_epoches, reg_coeff=reg_coeff)\n",
    "        score_val[f] = model.evaluate_acc(y[val], model.predict(x[val]))\n",
    "    return score_val, score_val.mean()\n",
    "\n",
    "regularization_strengths = np.array([0.001, 0.01, 0.1])\n",
    "\n",
    "\n",
    "#plot final performance graphs\n",
    "for strength in regularization_strengths:\n",
    "    model = MLP(relu, depth=2, m=256, l2 = True)\n",
    "    train_losses, train_accuracies, test_accuracies = model.fit(X_train, y_train, X_test, y_test,learning_rate=0.01,epochs=100, reg_coeff=strength)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), layout='constrained')\n",
    "    ax.plot(test_accuracies, label=\"Testing Set\")\n",
    "    ax.plot(train_accuracies, label = \"Training Set\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"Strength = {strength}\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    yh_train = model.predict(X_train)\n",
    "    yh_test = model.predict(X_test)\n",
    "    test_acc = model.evaluate_acc(y_test, yh_test)\n",
    "    train_acc = model.evaluate_acc(y_train, yh_train)\n",
    "    print(f\"Regularization Strength: \" + str(strength))\n",
    "    print(f\"Final test acc = {test_acc}\")\n",
    "    print(f\"Final test acc = {train_acc}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
